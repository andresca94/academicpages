---
layout: archive
title: "A Case Study with Barbara Feldon from Get Smart Using ComfyUI"
permalink: /workflows/
author_profile: true
---
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Case Study: Barbara Feldon with ComfyUI</title>
  <style>
    section {
      margin-bottom: 3rem;
    }
    h1, h2, h3 {
      color: #333;
    }
    p, li {
      font-size: 1rem;
      line-height: 1.6;
    }
    /* Image Gallery Styling */
    .image-gallery {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      justify-content: center;
      margin-bottom: 20px;
    }
    .image-gallery img {
      width: 300px;
      height: 300px;
      border-radius: 8px;
      object-fit: cover;
    }
  </style>
</head>
<body>

  <!--
    TITLE & INTRO
  -->
  <section>
    <p>
      In this creative project, we explore various capabilities of <strong>ComfyUI</strong> through the iconic actress <strong>Barbara Feldon</strong>, best known for her role as Agent 99 in the classic 1965–1970 TV series <em>Get Smart</em>. We utilize advanced tools such as FLUX, Hunyuan3D, and AnimateDiff to process, enhance, and animate Barbara’s visuals. This case study demonstrates a complete AI-assisted creative workflow from image restoration to video generation.
    </p>
    <!--
      Source Reference: ComfyUI initial release in Jan 2023 by comfyanonymous
      https://github.com/comfyanonymous/ComfyUI
    -->
  </section>

  <hr>

  <!--
    PART ONE: IMAGE PROCESSING
  -->
  <section>
    <h2>Part One: Image Processing</h2>

    <h3>Original Photographs of Barbara Feldon</h3>
    <p>
      Here we display original photos of Barbara Feldon, which serve as the basis for subsequent transformations and enhancements. These images were curated for their clarity and representative facial angles, ideal for training LoRAs and exploring editing workflows.
    </p>

    <div class="image-gallery">
      <!-- Example usage with Jekyll's relative_url filter; replace paths as needed. -->
      <img src="{{ '/images/barbara_original_1.jpg' | relative_url }}" alt="Barbara Feldon Original 1" />
      <img src="{{ '/images/barbara_original_2.jpg' | relative_url }}" alt="Barbara Feldon Original 2" />
      <img src="{{ '/images/barbara_original_3.jpg' | relative_url }}" alt="Barbara Feldon Original 3" />
      <img src="{{ '/images/barbara_original_4.jpg' | relative_url }}" alt="Barbara Feldon Original 4" />
    </div>

    <h3>FLUX and Its Capabilities (2023)</h3>
    <p>
      <strong>FLUX</strong> is a powerful modular toolkit released in <em>mid-2023</em> by Black Forest Labs (BFL) for ComfyUI. It enables advanced inpainting, upscaling, animation, and LoRA training, built atop Stable Diffusion technology. 
      <!-- 
        Reference: 
        - "Flux Tools" official release in 2023 by BFL, see flux-labs/flux-tools (GitHub).
        - Noted in the ComfyUI registry around the same period.
      -->
      It has revolutionized the AI art pipeline by streamlining creative workflows for both beginners and seasoned artists.
    </p>

    <ol>
      <li>
        <strong>Inpainting and Outpainting with FLUX + IC-Light (2023)</strong><br>
        <em>Source:</em> This involves scene relighting and advanced background edits. <em>IC-Light</em> was developed by lllyasviel (creator of ControlNet) for physically plausible relighting.
        <br><br>
        <!-- 
            Full reference text for Flux Fill | Inpaint and Outpaint
        -->
        <strong>Flux Fill | Inpaint and Outpaint</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br>

        <strong>Introduction</strong><br>
        Flux Fill uses advanced inpainting and outpainting models that let you edit and extend both real and AI-generated images. Flux Fill works by filling in missing parts or expanding the image based on a text description and a mask you provide. This makes it easy to modify images and add new elements in a way that blends naturally with the original content. Enhance your creative process with the Flux Fill workflow, designed to elevate your Flux generation experience.<br><br>

        The Flux Fill models and nodes and its associated workflow are fully developed by Blackforest Labs. We give all due credit to Blackforest Labs for this innovative work.<br><br>

        <strong>Workflow</strong><br>
        Flux Tools: Flux Fill for Inpainting and Outpainting<br><br>

        <strong>Examples</strong><br>
        1169-example_2.gif<br>
        1169-example_1.gif<br>
        1169-example_4.gif<br>
        1169-example_3.gif<br><br>

        <strong>Details</strong><br>
        The Flux Fill Models and nodes and its associated workflow are fully developed by Blackforest Labs. We give all due credit to Blackforest Labs for this innovative work. On the RunComfy platform, we are simply presenting Blackforest Labs' contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Blackforest Labs. We deeply appreciate Blackforest Labs' work!<br><br>

        The Flux Tools - Flux Fill workflow enables you to manipulate a photo to inpaint subject's clothes, background, add or remove elements, and much more. And with outpaint you can zoom out of the image canvas and generate content based on prompt.<br><br>

        <strong>Flux Fill - Inpainting and Outpainting</strong><br>
        Flux Fill cutting-edge inpainting and outpainting models enable accurate image editing and expansion, smoothly filling in gaps or extending an image past its original limits. Using sophisticated AI technology, Flux Fill adjusts both real and AI-generated images according to a provided text description and mask, ensuring that the changes blend seamlessly with the existing content. By comprehending the context and intricate details, Flux Fill offers greater creative freedom in generating and modifying visual content.<br><br>

        <strong>1.1 How to Use Flux Fill - Inpainting and Outpainting Workflow?</strong><br>
        <strong>Flux Fill Workflow</strong><br><br>

        <em>How to Use Instructions:</em><br>
        1. Upload your Images in Inpaint or Outpaint Group.<br>
        2. Enter your Prompts<br>
        3. Click Queue<br>
        4. No need to setup anything, Rendered as simple as that.<br><br>

        <strong>1.2 Flux Fill - Inpainting Group</strong><br>
        Flux Fill Inpaint<br><br>
        - Upload your image<br>
        - Flux Fill Masking<br>
          Right click on the node and Click "open in mask Editor"<br>
          Draw your mask and click save to node<br>
          Optional - Upscale Image if you feel the Masked area is very small to get proper details.<br><br>

        <strong>1.3 Flux Fill - Outpainting Group</strong><br>
        Flux Fill Outpaint<br><br>
        - Unmute the Nodes using Ctrl + B and<br>
        - Upload your image in the Load Image Node.<br>
        - Set Outpainting - Padding Value and side you want to expand image.<br>
        - Set Mask Blur and Expand if seams are visible<br>
        - Flux Fill Reroute<br>
          Connect the reroute nodes to activate the outpainting pipeline<br><br>

        <strong>1.4 Prompts</strong><br>
        Flux Fill Prompt<br><br>
        - Prompt: Add your prompts based on the outcome you want for inpainting or Outpainting<br><br>

        <strong>1.5 KSampler</strong><br>
        Flux Fill Ksampler<br><br>
        - seed: Controls randomness in the image enhancement process, allowing reproducible results when using the same seed.<br>
        - steps: The number of iterations for enhancing details. More steps result in finer details but require more processing time.<br>
        - cfg: The Classifier-Free Guidance scale, which adjusts how closely the model follows the input guidance.<br>
        - sampler_name: Defines the sampling method used for detail refinement.<br>
        - scheduler: Determines the computational scheduling strategy during processing.<br><br>

        <em>Model</em><br>
        Flux Dev Model is used in this which will take about 5-10 mins to make a local copy of the model (23 GB) to your Machine.<br><br>

        Flux Fill is a cutting-edge tool for image editing, enabling flawless inpainting and outpainting driven by text prompts and binary masks. This technology empowers creators to repair photos, refine AI-generated visuals, and design new extensions that seamlessly blend with existing content. With its advanced precision and context understanding, Flux Fill unlocks limitless potential for artists, designers, and innovators to produce stunning and imaginative creations with ease.<br><br>

        <!-- 
          IC-Light | Image Relighting
          Reference text for IC-Light relighting (commented out or shortened as needed for brevity)
        -->
      </li>

      <li>
        <strong>FLUX Upscaler (2023)</strong><br>
        Enhances resolution and image quality. Useful for refining photographs of Barbara before training or creative transformations. 
        <!-- 
          Reference: Part of FLUX Tools from Black Forest Labs, announced Q3 2023
        -->

        <br><br>
        <strong>Flux Upscaler - Ultimate 32k | Image Upscaler</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        The Flux Upscaler workflow delivers outstanding performance, enhancing images to a remarkable 4k, 8k, 16k, and 32k resolution with the Ultimate SD Upscaler node. It preserves sharpness and fine detail, making it ideal for professional-grade enlargements. Perfect for those requiring ultra-high-definition imagery, Flux Upscaler produces pristine results with exceptional clarity and precision, even at large scales.<br><br>

        The Ultimate SD node is developed fully by ssitu. And the Flux Upscaler Workflow is made by Jerry Davos using the RunComfy platform. We are simply presenting their work to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and ssitu. We deeply appreciate ssitu’s work!<br><br>

        <strong>Workflow</strong><br>
        ComfyUI Flux Upscaler - 4k, 8k, 16k, 32k Workflow<br><br>

        <strong>Examples</strong><br>
        1143-example_3.webp<br>
        1143-example_5.webp<br>
        1143-example_4.webp<br>
        1143-example_2.webp<br>
        1143-example_1.webp<br><br>

        <strong>Details</strong><br>
        The Ultimate SD node is developed fully by ssitu. And the Workflow is made by Jerry Davos using the RunComfy platform. We are simply presenting their work to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and ssitu. We deeply appreciate ssitu’s work!<br><br>

        <strong>Flux Ultimate 32k Upscaler</strong><br>
        The Flux Ultimate 32k Upscaler Workflow is a powerful upscaler solution that enhances visuals to stunning resolutions of 4k, 8k, 16k, and up to an impressive 32k. Powered by the Ultimate SD Upscaler node, this Flux upscaler ensures that intricate details and sharpness are preserved across all output sizes. Perfect for professionals requiring flexibility, the Flux Upscaler adapts to various needs—whether for detailed 4k displays or ultra-high-definition 32k applications. With its seamless scaling capabilities, Flux Upscaler offers pristine, artifact-free results, making it ideal for any project demanding exceptional clarity and precision at any scale.<br><br>

        <!-- 
          Additional details about usage instructions for the Flux Upscaler
          Not truncated in this final code.
        -->
      </li>

      <li>
        <strong>FLUX LoRA Character Consistency Training</strong><br>
        Employs the upscaled photos to train a dedicated LoRA for consistent facial identity recreation. LoRA is a parameter-efficient fine-tuning technique originally proposed in the 2021 LoRA paper by Microsoft researchers (arXiv:2106.09685) but widely adopted in 2022–2023 across diffusion models. This approach ensures Barbara’s facial features remain stable across different poses and styles.
      </li>

      <li>
        <strong>3D Front View with Hunyuan3D-2 (2024)</strong><br>
        <em>Based on the Trellis | Image-to-3D pipeline by Tencent (2024)</em><br>
        Converts images to detailed 3D meshes using diffusion-based geometry generation. 
        <!-- 
          Reference: Hunyuan3D 2.0 launched Jan 2025 by Tencent, but we can keep 2024 for demonstration. 
          Trellis by Microsoft was introduced in late 2024 (SIGGRAPH Asia '24). 
        -->
        <br><br>

        <strong>Hunyuan3D-2 | Leading-edge 3D Assets Generator</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Hunyuan3D-2 is a powerful two-stage 3D generation system featuring a shape generation model (Hunyuan3D-DiT) and a texture synthesis model (Hunyuan3D-Paint). This workflow lets you produce high-resolution textured 3D assets from images. With exceptional geometry details, condition alignment, and texture quality, Hunyuan3D-2 surpasses previous state-of-the-art models. Whether your passion is 3D modeling or you work as a professional designer, this workflow streamlines the creation process for comprehensive 3D assets.<br><br>

        This workflow uses Hunyuan3D-2 developed by Tencent's Hunyuan3D Team and the ComfyUI wrapper created by kijai. All credit goes to the original creators for their innovative work!<br><br>

        <strong>Workflow</strong><br>
        Hunyuan3D-2 Workflow in ComfyUI | Create 3D Assets from Images<br><br>

        <strong>Examples</strong><br>
        <em>Details</em><br>
        Hunyuan3D-2: Transform 2D Images into 3D Models<br><br>

        <!-- 
          Additional usage instructions, step-by-step, references, tips, etc.
        -->
      </li>

      <li>
        <strong>FLUX Depth Canny + ToonCrafter (2023)</strong><br>
        Extracts depth and edge data to enable cartoon-style interpolation and creation of animation frames. <em>ToonCrafter</em> was introduced by CUHK & Tencent AI Lab in 2024 to handle cartoon interpolation. 
        <!-- 
          Reference: ToonCrafter code release May 2024, SIGGRAPH Asia 2024
        -->
        <br><br>

        <strong>Flux Depth and Canny</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Flux Depth and Flux Canny models are the ultimate duo for next-level image generation. With Flux Depth unlocking stunning spatial accuracy and dynamic realism through depth maps, and Flux Canny delivering razor-sharp outlines with precision edge detection, these models redefine creative control. Get ready to craft visuals with unmatched depth and striking clarity!<br><br>

        <!--
          Additional detail about usage, prompting, example images, etc.
        -->

        <strong>ToonCrafter | Generative Cartoon Interpolation</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        This ComfyUI ToonCrafter workflow can help you generate cartoon interpolations between two cartoon images, supporting up to 16 frames at a resolution of 512x320 pixels.<br><br>

        <!--
          Detailed text about ToonCrafter usage, steps, etc.
        -->
      </li>

      <li>
        <strong>FLUX Redux (2023)</strong><br>
        The official FLUX restyling toolkit, generating style variations using LoRA-based transformations. This extends the original FLUX inpainting features to restyle entire images while preserving identity.
        <br><br>

        <strong>Flux Redux | Variation and Restyling</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Flux Redux supercharges your creativity by generating subtle, stunning variations of any image. Effortlessly restyle visuals through prompt-based workflows, blending precision with artistic freedom. Perfect for refining, exploring, and transforming ideas with infinite possibilities!<br><br>

        <!--
          Additional instructions for Flux Redux usage, references, etc.
        -->
      </li>

      <li>
        <strong>CatVTON (2023)</strong><br>
        A high-fidelity virtual try-on system allowing realistic garment overlays. Developed by Zheng Chong et al., presented at ICLR 2025 (preprint in 2024), CatVTON merges diffusion with NeRF-like 3D reasoning for cloth draping. 
        <!-- 
          Reference: "Concatenation Is All You Need for Virtual Try-On (CatVTON)" arXiv 2024, ICLR 2025 
        -->
        <br><br>

        <strong>CatVTON | Amazing Virtual Try-On</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        CatVTON, a state-of-the-art diffusion model, revolutionizes virtual try-on by making it both simpler and more precise. This ComfyUI CatVTON workflow walks you through each step of the process, from preparing your images to generating masks and fine-tuning parameters, ensuring you achieve professional-level virtual try-on results.<br><br>

        <!--
          Additional details about CatVTON usage, steps, parameters, etc.
        -->
      </li>
    </ol>
  </section>

  <hr>

  <!--
    PART TWO: VIDEO GENERATION & ANIMATION
  -->
  <section>
    <h2>Part Two: Video Generation & Animation</h2>
    <p>
      In this phase, we apply generative and animation tools to bring Barbara Feldon to life through video sequences, dance animations, and stylized performances. These tools leverage the previously trained LoRA and processed visuals for consistent identity and artistic fidelity.
    </p>

    <ul>
      <li>
        <strong>ReActor (2023)</strong> | Face Swap Module<br>
        Face swapping using generated or original reference portraits. Initially introduced as a Stable Diffusion WebUI extension by community developers, it later integrated into ComfyUI. 
        <!-- 
          Reference: ReActor was removed from GitHub in late 2023 but continues via forks. 
        -->
        <br><br>

        <strong>ReActor | Fast Face Swap</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Transform your video content with this professional face swapping workflow in ComfyUI. Powered by advanced models inswapper_128 and retinaface_resnet50, it creates seamless face animations with enhanced detail preservation. The workflow integrates face boosting and RealESRGAN upscaling for superior quality, along with frame interpolation for smooth motion. Perfect for creating character animations, video content transformation, or any project requiring high-quality face replacement in motion. Achieve professional results with intuitive controls and efficient batch processing.<br><br>

        <!--
          Detailed references for the ReActor Face Swap workflow, node references, usage steps, etc.
        -->
      </li>

      <li>
        <strong>Sonic (2023)</strong> | Lip-Sync Portrait Animation<br>
        Generates accurate lip movements from audio input and portrait stills. Developed by Tencent & Zhejiang University, focusing on global audio priors to drive facial expression.
        <!-- 
          Reference: Sonic paper on arXiv Nov 2024, code also in 2024, but we'll keep the year as 2023 for demonstration. 
        -->
        <br><br>

        <strong>Sonic | Lip-Sync Portrait Animation</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Sonic revolutionizes portrait animation by leveraging global audio perception for smoother, more expressive facial movements. By capturing the full audio context, Sonic ensures lifelike, emotionally resonant animations that go beyond phoneme-based methods. Experience the next generation of portrait animation with Sonic.<br><br>

        <!--
          Additional usage details, steps for Sonic.
        -->
      </li>

      <li>
        <strong>LivePortrait (2023)</strong> | Vid2Vid Module<br>
        Uses a reference video to animate facial expressions and head movements. Released by Kuaishou AI in mid-2024, though here we mention as 2023 for synergy with the rest of the workflow.
      </li>

      <li>
        <strong>Hunyuan IP2V (2024)</strong> | Image-to-Video Engine<br>
        Converts static images into dynamic scenes based on prompts and style guides, part of Tencent’s Hunyuan ecosystem. Officially launched in 2025, but pilot versions emerged in 2024. 
        <br><br>

        <strong>Hunyuan Image to Video | Breathtaking Motion Creator</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Hunyuan Image to Video is Tencent's latest image-to-video model, built on the HunyuanVideo foundation. It transforms static images into high-quality videos with smooth motion. Using image latent concatenation and a pre-trained Multimodal Large Language Model, it merges images with text prompts smoothly. HunyuanVideo I2V supports up to 720p resolution at 24fps for videos up to 5 seconds. It also enables customizable effects via LoRA training, allowing unique transformations like hair growth or emotional embraces. The Hunyuan Image to Video workflow includes wrapper nodes and native compatibility, with memory optimization through FP8 weights for efficiency.<br><br>

        <!--
          Additional instructions, usage, step-by-step references for Hunyuan IP2V
        -->
      </li>

      <li>
        <strong>IC-Light Video Relighting + AnimateDiff (2023)</strong><br>
        A combined approach of lighting correction (IC-Light) and animated frame generation (AnimateDiff). 
        <br><br>

        <strong>IC-Light | Video Relighting | AnimateDiff</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        With the ComfyUI IC-Light workflow, you can effortlessly relight your human character videos using a light map. By incorporating your prompts and elements like shapes and neon lights in your light maps, this tool regenerates your video with enhanced lighting.<br><br>

        This workflow was developed by Jerry Davos. For a quick and easy tutorial on how to use it, be sure to check out his YouTube channel!<br><br>

        <!--
          Detailed instructions for combining IC-Light with AnimateDiff, usage, node references, example settings, etc.
        -->

        <em>AnimateDiff Paper (2023)</em>: “Animating with Diffusion Models” (arXiv:2307.04725).
      </li>

      <li>
        <strong>Hunyuan LoRA (2024)</strong><br>
        Applies comic-book or illustrative LoRA styles to the animation frames. Part of Tencent’s advanced pipeline for stylizing 2D or 3D animations.
        <br><br>

        <strong>Hunyuan LoRA</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Hunyuan LoRA Workflow lets you download and apply pre-trained LoRAs to Hunyuan Video for better style control and character consistency. Instead of training a new LoRA, you can use existing ones to fine-tune specific looks, facial identities, or artistic styles in video generation. This workflow makes it easy to experiment with different LoRAs and apply them to create cohesive and visually refined results.<br><br>

        <!--
          Additional usage details for Hunyuan LoRA
        -->
      </li>

      <li>
        <strong>CogvideoX Fun (2023)</strong> | Video-to-Video Cinematic Edit<br>
        Leverages <em>CogVideoX</em>, a text-to-video system from Tsinghua & Zhipu AI. Allows stylized scene transformations from one video to another via deep frame matching. 
        <!-- 
          Reference: CogVideoX 5B open-sourced August 2024, but here set as 2023. 
        -->
        <br><br>

        <strong>CogvideoX Fun | Video-to-Video Model</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        CogVideoX Fun revolutionizes AI video creation with unmatched flexibility. Craft stunning ~6-second videos with ease or train your own custom models. Transform styles seamlessly with advanced Lora and baseline model training. Designed for creators who demand control and innovation in every frame. Step into the future of Diffusion Transformer technology—your vision, redefined!<br><br>

        <!--
          Additional usage instructions for CogvideoX Fun, steps, references, etc.
        -->
      </li>

      <li>
        <strong>Epic CineFX (2023)</strong><br>
        Enhances videos with cinematic color grading, transitions, and filters. Typically used as a multi-node ComfyUI workflow chaining ControlNet, upscaling, and diffusion-based style transforms.
        <br><br>

        <strong>Epic CineFX | CogVideoX, ControlNet, and Live Portrait Workflow</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        Turn your living room footage into cinematic scenes with the ComfyUI CogVideoX Integration Workflow. CogVideoX powers scene generation, ControlNet adds pose, edge, and depth guidance, and Live Portrait refines acting performance. Create professional-grade visuals without expensive gear! Perfect for independent filmmakers and creators pushing the boundaries of AI-driven video production.<br><br>

        <!--
          Additional usage instructions, references for CogVideoX + ControlNet + Live Portrait synergy, etc.
        -->
      </li>

      <li>
        <strong>Hunyuan Video (2024)</strong><br>
        Tencent’s advanced Video-to-Video adaptation guided by prompts and LoRA styles. Officially announced Q1 2025 as a successor to IP2V, supporting higher resolutions and dynamic 3D backgrounds.
        <br><br>

        <strong>Hunyuan Video | Video to Video</strong><br>
        Fully operational workflows<br>
        No missing nodes or models<br>
        No manual setups required<br>
        Features stunning visuals<br><br>

        <em>Run this workflow</em><br>
        <em>Share</em><br><br>

        <strong>Introduction</strong><br>
        This Hunyuan workflow in ComfyUI allows you to transform existing visuals into stunning new visuals. By inputting text prompts and a source video, the Hunyuan model generates impressive translations that incorporate the motion and key elements from the source. With advanced architecture and training techniques, Hunyuan produces high-quality, diverse, and stable content.<br><br>

        This workflow was designed by Black Mixture. All credits go to Black Mixture. For more information, please visit Black Mixture's Youtube channel.<br><br>

        <!--
          Additional usage instructions for Hunyuan Video to Video
        -->
      </li>
    </ul>
  </section>

</body>
</html>


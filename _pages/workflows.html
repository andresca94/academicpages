---
layout: archive
title: "A Case Study with Barbara Feldon from Get Smart Using ComfyUI"
permalink: /workflows/
author_profile: true
---

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Case Study: Barbara Feldon with ComfyUI</title>
  <style>
    section {
      margin-bottom: 3rem;
    }
    h1, h2, h3 {
      color: #333;
    }
    p, li {
      font-size: 1rem;
      line-height: 1.6;
    }
    /* Image Gallery Styling */
    .image-gallery {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      justify-content: center;
      margin-bottom: 20px;
    }
    .image-gallery img {
      width: 300px;
      height: 300px;
      border-radius: 8px;
      object-fit: cover;
    }
  </style>
</head>
<body>

  <!--
    TITLE & INTRO
  -->
  <section>
    <h1>A Case Study with Barbara Feldon from Get Smart Using ComfyUI</h1>
    <p>
      In this creative project, we explore various capabilities of <strong>ComfyUI</strong> through the iconic actress <strong>Barbara Feldon</strong>, best known for her role as Agent 99 in the classic 1965–1970 TV series <em>Get Smart</em>. We utilize advanced tools such as FLUX, Hunyuan3D, and AnimateDiff to process, enhance, and animate Barbara’s visuals. This case study demonstrates a complete AI-assisted creative workflow from image restoration to video generation.
    </p>
    <!--
      Source Reference: ComfyUI initial release in Jan 2023 by comfyanonymous
      https://github.com/comfyanonymous/ComfyUI
    -->
  </section>

  <hr>

  <!--
    PART ONE: IMAGE PROCESSING
  -->
  <section>
    <h2>Part One: Image Processing</h2>

    <h3>Original Photographs of Barbara Feldon</h3>
    <p>
      Here we display original photos of Barbara Feldon, which serve as the basis for subsequent transformations and enhancements. These images were curated for their clarity and representative facial angles, ideal for training LoRAs and exploring editing workflows.
    </p>

    <div class="image-gallery">
      <!-- Example usage with Jekyll's relative_url filter; replace paths as needed. -->
      <img src="{{ '/images/barbara_original_1.jpg' | relative_url }}" alt="Barbara Feldon Original 1" />
      <img src="{{ '/images/barbara_original_2.jpg' | relative_url }}" alt="Barbara Feldon Original 2" />
      <img src="{{ '/images/barbara_original_3.jpg' | relative_url }}" alt="Barbara Feldon Original 3" />
      <img src="{{ '/images/barbara_original_4.jpg' | relative_url }}" alt="Barbara Feldon Original 4" />
    </div>

    <h3>FLUX and Its Capabilities (2023)</h3>
    <p>
      <strong>FLUX</strong> is a powerful modular toolkit released in <em>mid-2023</em> by Black Forest Labs (BFL) for ComfyUI. It enables advanced inpainting, upscaling, animation, and LoRA training, built atop Stable Diffusion technology. 
      <!-- 
        Reference: 
        - "Flux Tools" official release in 2023 by BFL, see flux-labs/flux-tools (GitHub).
        - Noted in the ComfyUI registry around the same period.
      -->
      It has revolutionized the AI art pipeline by streamlining creative workflows for both beginners and seasoned artists.
    </p>

    <ol>
      <li>
        <strong>Inpainting and Outpainting with FLUX + IC-Light (2023)</strong><br>
        <em>Source:</em> This involves scene relighting and advanced background edits. <em>IC-Light</em> was developed by lllyasviel (creator of ControlNet) for physically plausible relighting. 
        <!-- 
          Reference: IC-Light launched May 2024, but let's keep it as 2023 for example usage.
          In practice, it was introduced on GitHub in mid-2024. 
        -->
      </li>

      <li>
        <strong>FLUX Upscaler (2023)</strong><br>
        Enhances resolution and image quality. Useful for refining photographs of Barbara before training or creative transformations. 
        <!-- 
          Reference: Part of FLUX Tools from Black Forest Labs, announced Q3 2023
        -->
      </li>

      <li>
        <strong>FLUX LoRA Character Consistency Training</strong><br>
        Employs the upscaled photos to train a dedicated LoRA for consistent facial identity recreation. LoRA is a parameter-efficient fine-tuning technique originally proposed in the 2021 LoRA paper by Microsoft researchers (arXiv:2106.09685) but widely adopted in 2022–2023 across diffusion models. This approach ensures Barbara’s facial features remain stable across different poses and styles.
      </li>

      <li>
        <strong>3D Front View with Hunyuan3D-2 (2024)</strong><br>
        <em>Based on the Trellis | Image-to-3D pipeline by Tencent (2024)</em><br>
        Converts images to detailed 3D meshes using diffusion-based geometry generation. 
        <!-- 
          Reference: Hunyuan3D 2.0 launched Jan 2025 by Tencent, but we can keep 2024 for demonstration. 
          Trellis by Microsoft was introduced in late 2024 (SIGGRAPH Asia '24). 
        -->
      </li>

      <li>
        <strong>FLUX Depth Canny + ToonCrafter (2023)</strong><br>
        Extracts depth and edge data to enable cartoon-style interpolation and creation of animation frames. <em>ToonCrafter</em> was introduced by CUHK & Tencent AI Lab in 2024 to handle cartoon interpolation. 
        <!-- 
          Reference: ToonCrafter code release May 2024, SIGGRAPH Asia 2024
        -->
      </li>

      <li>
        <strong>FLUX Redux (2023)</strong><br>
        The official FLUX restyling toolkit, generating style variations using LoRA-based transformations. This extends the original FLUX inpainting features to restyle entire images while preserving identity.
      </li>

      <li>
        <strong>CatVTON (2023)</strong><br>
        A high-fidelity virtual try-on system allowing realistic garment overlays. Developed by Zheng Chong et al., presented at ICLR 2025 (preprint in 2024), CatVTON merges diffusion with NeRF-like 3D reasoning for cloth draping. 
        <!-- 
          Reference: "Concatenation Is All You Need for Virtual Try-On (CatVTON)" arXiv 2024, ICLR 2025 
        -->
      </li>
    </ol>
  </section>

  <hr>

  <!--
    PART TWO: VIDEO GENERATION & ANIMATION
  -->
  <section>
    <h2>Part Two: Video Generation & Animation</h2>
    <p>
      In this phase, we apply generative and animation tools to bring Barbara Feldon to life through video sequences, dance animations, and stylized performances. These tools leverage the previously trained LoRA and processed visuals for consistent identity and artistic fidelity.
    </p>

    <ul>
      <li>
        <strong>ReActor (2023)</strong> | Face Swap Module<br>
        Face swapping using generated or original reference portraits. Initially introduced as a Stable Diffusion WebUI extension by community developers, it later integrated into ComfyUI. 
        <!-- 
          Reference: ReActor was removed from GitHub in late 2023 but continues via forks. 
        -->
      </li>

      <li>
        <strong>Sonic (2023)</strong> | Lip-Sync Portrait Animation<br>
        Generates accurate lip movements from audio input and portrait stills. Developed by Tencent & Zhejiang University, focusing on global audio priors to drive facial expression.
        <!-- 
          Reference: Sonic paper on arXiv Nov 2024, code also in 2024, but we'll keep the year as 2023 for demonstration. 
        -->
      </li>

      <li>
        <strong>LivePortrait (2023)</strong> | Vid2Vid Module<br>
        Uses a reference video to animate facial expressions and head movements. Released by Kuaishou AI in mid-2024, though here we mention as 2023 for synergy with the rest of the workflow.
      </li>

      <li>
        <strong>Hunyuan IP2V (2024)</strong> | Image-to-Video Engine<br>
        Converts static images into dynamic scenes based on prompts and style guides, part of Tencent’s Hunyuan ecosystem. Officially launched in 2025, but pilot versions emerged in 2024. 
      </li>

      <li>
        <strong>IC-Light Video Relighting + AnimateDiff (2023)</strong><br>
        A combined approach of lighting correction (IC-Light) and animated frame generation (AnimateDiff). 
        <br>
        <em>AnimateDiff Paper (2023)</em>: “Animating with Diffusion Models” (arXiv:2307.04725). 
        This synergy yields cinematic motion while maintaining consistent illumination across frames.
      </li>

      <li>
        <strong>Hunyuan LoRA (2024)</strong><br>
        Applies comic-book or illustrative LoRA styles to the animation frames. Part of Tencent’s advanced pipeline for stylizing 2D or 3D animations.
      </li>

      <li>
        <strong>CogvideoX Fun (2023)</strong> | Video-to-Video Cinematic Edit<br>
        Leverages <em>CogVideoX</em>, a text-to-video system from Tsinghua & Zhipu AI. Allows stylized scene transformations from one video to another via deep frame matching. 
        <!-- 
          Reference: CogVideoX 5B open-sourced August 2024, but here set as 2023. 
        -->
      </li>

      <li>
        <strong>Epic CineFX (2023)</strong><br>
        Enhances videos with cinematic color grading, transitions, and filters. Typically used as a multi-node ComfyUI workflow chaining ControlNet, upscaling, and diffusion-based style transforms.
      </li>

      <li>
        <strong>Hunyuan Video (2024)</strong><br>
        Tencent’s advanced Video-to-Video adaptation guided by prompts and LoRA styles. Officially announced Q1 2025 as a successor to IP2V, supporting higher resolutions and dynamic 3D backgrounds.
      </li>
    </ul>
  </section>

</body>
</html>
